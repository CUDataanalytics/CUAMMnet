---
title: "Introduction to Malaria Modeling Skills in R using Machine Learning Algorithms"
author: "D.K.MURIITHI"
date: "2024-07-25"
output:
  html_document:  
  word_document: default
  pdf_document: default
  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.height = 4,
	fig.width = 6,
	message = FALSE,
	warning = FALSE,
	comment = NA)
```

## Introduction to Malaria modeling Skills in R using Machine Learning Techniques

Machine learning (ML) is a subfield of artificial intelligence (AI) that focuses on the development of algorithms capable of learning and making predictions or decisions based on inputs and data (Sarker, 2021). 

*   The process of developing a machine learning model begins with the collection and preprocessing of data

*   Data preprocessing includes data cleaning, transformation, organization, imputation, and labelling

#Types of Machine Learning;

1. Supervised Learning: The model is trained on labeled data. Examples include regression and classification tasks.

2. Unsupervised Learning: The model is trained on unlabeled data, identifying patterns and relationships. Examples include clustering and association tasks.

3. Semi-Supervised Learning: Combines both labeled and unlabeled data to improve learning accuracy.

4. Reinforcement Learning: The model learns through trial and error, receiving rewards or penalties based on actions performed

# Get started---------------------

## Confirmation and setting of working directory

```{r}
setwd("C:\\Users\\Prof DK\\Desktop\\TMMS2024")
```

## Installation and loading of necessary packages/libraries

# Loading libraries
```{r}
library(caret) #for training machine learning models
library(psych) ##for description of  data
library(ggplot2) ##for data visualization
library(caretEnsemble)##enables the creation of ensemble models
library(tidyverse) ##for data manipulation
library(mlbench)  ## For benchmarking ML Models
library(flextable) ## To create and style tables
library(mltools) #for hyperparameter tuning
library(tictoc) #for determining the time taken for a model to run
library(ROSE)  ## for random oversampling
library(smotefamily) ## for smote sampling
library(ROCR) ##For ROC curve
library(pROC) ## For visualizing, smoothing, and comparing ROC curves
library(e1071) ## For statistical modeling and  machine learning tasks
library(class) ## For classification using k-Nearest Neighbors and other methods
library(caTools) ## For splitting data into training and testing sets
library(MASS) ## Provides plotting functions and datasets
library(ISLR) ## for practical applications of statistical learning methods
library(boot) ## Useful for performing bootstrap resampling
library(cvTools) ## Contains functions for cross-validation, bootstrapping, and other resampling methods
```


## Loading the given Malaria data

```{r}
mdata = read.csv("final_malaria_survey_data.csv", header = TRUE)
head(mdata,4)
```

```{r}
dim(mdata)      ## View the Dimension of the Data
#names(mdata)     ## View the variable/features/column names
#summary(mdata)    ## Descriptive Statistics
#describe(mdata)   ## Descriptive Statistics
#sum(is.na(mdata))  ## Check for missing data
```

## Note: For the purpose of this training: It is assumed that the data is already clean and preprocessed 

# Plot Target Variable
```{r}
plot(factor(Malaria.Test), 
     names= c('Negative', 'Positive'), 
     col=c("green","red"), ylim=c(0, 3000), 
     ylab='Respondent', xlab='Malaria Diagnosis')
#box()
```
#Or use ggplot 

```{r}
ggplot(mdata, aes(x = Malaria.Test, fill = Malaria.Test)) + 
  geom_bar() + 
  labs(x = "Malaria Test", y = "Respondent") +
  theme_classic()
```

#Check for zero variance predictors:

```{r}
nzv <- nearZeroVar(mdata[,-14], saveMetrics = TRUE) ## Function called nearZeroVar and captures its output in the variable nzv
nzv
```

# The purpose of this code is to identify features in a dataset with near zero variance. Here's a breakdown of what each part does:

  * mdata[,-14]: This part selects all columns of the data in mdata and excludes the last 14 column.

  * nearZeroVar: This is likely a user-defined function that performs the main analysis

  * saveMetrics = TRUE: This argument instructs the nearZeroVar function to save additional metrics during its analysis               (potentially standard deviation for each feature).

  * print(nzv): This line simply prints the output of the nearZeroVar function, which is a list of column names that have near        zero variance.

# In essence, this code helps you clean your data by finding features that barely change across your observations. These features might not be informative for your analysis and can be removed to improve the efficiency of your models.

   * The results above show that there is no feature with zero variance

# Remove nzv
```{r}
mdata1 <- mdata[, !nzv$nzv] ## Removing features with little to no variation
dim(mdata1)
```
# Reasons to Remove NZV Features

* You can prevent them from interfering with the learning process of algorithms and potentially improve model accuracy

* Reduces computational cost: Fewer features means less computation needed for training models

## DATA PARTITION FOR MACHINE LEARNING

```{r}
set.seed(123)
index = sample(2, nrow(mdata1),replace =T, prob=c(0.70,0.30))
train = mdata1[index ==1,]
test = mdata1[index ==2,]
```

# Get the dimensions of your train and test data
```{r}
dim(train)
dim(test)
```

# Now Let's train some machine learning models using package caret

   * The caret R package (short for Classification and regression Training) to carry out machine learning tasks in      RStudio

   * The caret package offers a range of tools and models for classification and regression machine learning                          problems(Kuhn et al. 2021)

   * In fact, it offers over 239 different machine learning models from which to choose. 

   * Don’t worry, we don’t expect you to use them all!

## VIEW THE MODELS IN CARET

```{r}
models= getModelInfo()
#names(models)
```

# Prepare training scheme for cross-validation 

# Cross-validation
   *This involves splitting the data into multiple subsets (folds), training the model on some folds, and testing it on the            remaining fold. The process is repeated for each fold
# Repeated cross-validation 
   *This involves performing cross-validation multiple times to reduce the variability of the results
# Leave-One-Out Cross-Validation (LOOCV) 
   *LOOCVis a special case of cross-validation where the number of folds equals the number of data points. Each data point is used     once as a test set

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=5)
```


```{r}
#models= getModelInfo()
#names(models)
```
# TRAIN OR BUILD MACHINE LEARNING MODELS

# Train a Support Vector Machine (SVM) model
```{r}
set.seed(123)
tic()
SvmModel <- train(factor(Malaria.Test)~., data=train, method="svmRadial", trControl=control, na.action = na.omit)
toc()
SvmModel
Svmpred= predict(SvmModel,newdata = test)
SVM_CM<- confusionMatrix(Svmpred,as.factor(test$Malaria.Test), positive = "Positive", mode="everything")
SVM_CM
M1 <- SVM_CM$byClass[c(1, 2, 5, 7, 11)]
M1
```

Let's break down svm code what each line does:

* tic(): This function starts measuring the execution time. It's a placeholder to track how long the next line takes to run.

* toc(): This function stops measuring the execution time and presumably prints the elapsed time for training the model.

* train:  is a function from a machine learning library in R (like caret) used to train models.

* factor(Malaria.Test)~. defines the formula for the model. 
  **Here, "Malaria.Test" is the target variable (categorical, converted to factors using factor()), and 
  **the tilde (~) indicates we're predicting it based on all other features in the "train" data set.

* data=train specifies the training data set.

* method="svmRadial" defines the model type as a Support Vector Machine (SVM) with a Radial kernel.

* trControl=control sets the training parameters likely defined in the control object.

* na.action = na.omit tells the function to omit rows with missing values (NA) during training.

* The entire line trains the model and stores it in the variable SvmModel.

* SvmModel: This line by itself doesn't execute any code. It just prints the contents of the SvmModel variable, which likely contains the trained SVM model object. This can be useful for further analysis or prediction using the trained model.

* Overall, this code snippet trains a Radial SVM model to predict the presence of severe malaria based on features in the "train" data set. It sets a random seed for reproducibility, measures the training time, and then stores the trained model for further use.

* mode='everything' tells the function to calculate all metrics supported by the confusionMatrix function, including precision, recall, F1-score, and more.


# Train a Random Forest model
```{r}
set.seed(123)
tic()
RFModel <- train(factor(Malaria.Test)~., data=train, method="rf", trControl=control, na.action = na.omit)
toc()
RFModel
RFpred=predict(RFModel,newdata = test)
RF_CM<- confusionMatrix(RFpred,as.factor(test$Malaria.Test), positive = "Positive", mode='everything')
RF_CM
M2<- RF_CM$byClass[c(1, 2, 5, 7, 11)]
M2
```
# Plotting Random Forest confusion matrix
```{r}
fourfoldplot(RF_CM$table, col=rainbow(4), main="RF Confusion Matrix") ##RF Confusion Matrix 4fold plot
```

# Show relative importance of features

```{r}
var_imp <-varImp(RFModel)
var_imp$Variable <- rownames(var_imp)
rownames(var_imp) <- NULL

ggplot(var_imp, aes(x = reorder(Variable, Importance), y = importance)) +
  geom_bar(stat = "identity", fill = "tomato") +
  coord_flip() +
  xlab("Variable") +
  ylab("Importance") +
  ggtitle("Feature Importance Plot for RF Model")
```


# Create ROC curve for RF model
```{r}
# Make predictions on the test set using type='prob'
predrf <- predict(RFModel, newdata = test, type = "prob")
# Create a prediction object needed by ROCR
pred_rf <- prediction(predrf[, "Positive"], test$Malaria.Test)
# Calculate performance measures like ROC curve
perf_rf <- performance(pred_rf, "tpr", "fpr")
# Plot the ROC curve
plot(perf_rf, colorize = TRUE, main = "ROC Curve-Random Forest")
# Compute AUC
auc_value <- performance(pred_rf, "auc")@y.values[[1]]
auc_label <- paste("AUC =", round(auc_value, 2))
# Add AUC value as text on the plot
text(0.5, 0.3, auc_label, col = "blue", cex = 1.5)  # Adjust position
```

# Receiver Operating Characteristic (ROC) Curve

  *The ROC curve is a graphical representation of the performance of a classification model at all classification thresholds. The    curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR).

  *True Positive Rate (TPR): Also known as sensitivity or recall, it is the ratio of correctly predicted positive observations to    the actual positives. TPR = TP / (TP + FN), where TP is True Positives and FN is False Negatives.

  *False Positive Rate (FPR): It is the ratio of incorrectly predicted positive observations to the actual negatives. FPR = FP /     (FP + TN), where FP is False Positives and TN is True Negatives.

  *The AUC is the area under the ROC curve. It ranges from 0 to 1, with higher values indicating better model performance. An AUC    of 0.5 suggests no discrimination (i.e., the model is no better than random guessing), while an AUC of 1.0 indicates perfect      discrimination.

  *AUC = 0.99: This indicates that the Random Forest classifier has an excellent ability to distinguish between the positive and     negative classes. An AUC of 0.99 is very close to 1.0, suggesting that the model has a high true positive rate and a low false    positive rate.
  
# Train the decision Tree

```{r}
set.seed(123)
tic()
DTModel <- train(factor(Malaria.Test)~., data=train, method="rpart", trControl=control)
toc()
DTModel
DTpred=predict(DTModel,newdata = test)
DT.cM<- confusionMatrix(DTpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
DT.cM
### Prediction
T2<- DT.cM$byClass[c(1, 2, 5, 7, 11)]
T2
```

### plotting confusion matrix
```{R}
DT.cM$table
fourfoldplot(DT.cM$table, col=rainbow(4), main="Imbalanced DT Confusion Matrix")
plot(varImp(DTModel, scale=T))
```


# Train an Logisitic Regression model

```{r}
set.seed(123)
logRegModel <- train(factor(Malaria.Test)~., data=train, method="glm", trControl=control, na.action = na.omit)
logRegModel
logRegpred=predict(logRegModel,newdata = test)
logReg_CM<- confusionMatrix(logRegpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
logReg_CM
M3<- logReg_CM$byClass[c(1, 2, 5, 7, 11)]
M3

#plotting confusion matrix
logReg_CM$table
fourfoldplot(logReg_CM$table, col=rainbow(4), main="LR Confusion Matrix")
plot(varImp(logRegModel, scale=T))

# Make predictions on the test set using type='prob'
predlogReg <- predict(logRegModel, newdata = test, type = "prob")
# Create a prediction object needed by ROCR
pred_logReg <- prediction(predlogReg[, "Positive"], test$Malaria.Test)
# Calculate performance measures like ROC curve
perf_logReg <- performance(pred_logReg, "tpr", "fpr")
# Plot the ROC curve
plot(perf_logReg, colorize = TRUE, main = "ROC Curve-Logistic Regression")
# Compute AUC
auc_value <- performance(pred_logReg, "auc")@y.values[[1]]
auc_label <- paste("AUC =", round(auc_value, 2))
# Add AUC value as text on the plot
text(0.5, 0.3, auc_label, col = "blue", cex = 1.5)  # Adjust position and other text parameters as needed
```

# Train a k- Nearest Neigbour model

```{r}
set.seed(123)
knnModel <- train(factor(Malaria.Test)~., data = train, method ="knn", trControl = control)
#knnModel
knnpred = predict(knnModel,newdata = test)
knn_CM<- confusionMatrix(knnpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M4<- knn_CM$byClass [c(1, 2, 5, 7, 11)]
M4
#plotting confusion matrix
knn_CM$table
fourfoldplot(knn_CM$table, col=rainbow(4), main="KNN Confusion Matrix")
```

## Train a Neural Net model

```{r}
set.seed(123)
tic()
nnModel <- train(factor(Malaria.Test)~., data=train, method="nnet", trControl=control)
toc()
nnModel
nnpred=predict(nnModel,newdata = test)
nn_CM<- confusionMatrix(nnpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M5<- nn_CM$byClass[c(1, 2, 5, 7, 11)]
M5
#plotting confusion matrix
nn_CM$table
fourfoldplot(nn_CM$table, col=rainbow(4), main="Neural Network Confusion Matrix")
plot(varImp(nnModel, scale=T))
```
# Train a Naive Bayes model

```{r}
set.seed(123)
NBModel <- train(factor(Malaria.Test)~., data=train, method="nb",trControl=control)
NBModel
NBpred=predict(NBModel,newdata = test)
NB_CM<- confusionMatrix(NBpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M6<- NB_CM$byClass[c(1, 2, 5, 7, 11)]
M6
#plotting confusion matrix
NB_CM$table
fourfoldplot(NB_CM$table, col=rainbow(4), main="Naive Bayes Confusion Matrix")
#plot(varImp(NBModel, scale=T))
```

## Train a Linear Discriminant Analysis model

```{r}
set.seed(123)
LDAModel <- train(factor(Malaria.Test)~., data=train, method="lda", trControl=control)
LDAModel
LDApred=predict(LDAModel,newdata = test)
LDA_CM<- confusionMatrix(LDApred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M7<- LDA_CM$byClass[c(1, 2, 5, 7, 11)]
M7
#plotting confusion matrix
LDA_CM$table
fourfoldplot(LDA_CM$table, col=rainbow(4), main="LDA Confusion Matrix")
#plot(varImp(LDAModel, scale=T))
```

# Train a Linear Vector Quantization model
```{r}
set.seed(123)
LVQModel <- train(factor(Malaria.Test)~., data=train, method="lvq", trControl=control)
LVQModel
LVQpred=predict(LVQModel,newdata = test)
LVQ_CM<- confusionMatrix(LVQpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
LVQ_CM
M8<- LVQ_CM$byClass[c(1, 2, 5, 7, 11)]
M8
#plotting confusion matrix
LVQ_CM$table
fourfoldplot(LVQ_CM$table, col=rainbow(4), main="LDA Confusion Matrix")
#plot(varImp(LVQModel, scale=T))
```

# Train a Bagging model

```{r}
set.seed(123)
tic()
bagModel <- train(factor(Malaria.Test)~., data=train, method="treebag", trControl=control)
toc()
bagModel
bagpred=predict(bagModel,newdata = test)
bag_CM<- confusionMatrix(bagpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M9<- bag_CM$byClass[c(1, 2, 5, 7, 11)]
M9
#plotting confusion matrix
bag_CM$table
fourfoldplot(bag_CM$table, col=rainbow(4), main="Bagging Confusion Matrix")
plot(varImp(bagModel, scale=T))
```
# Train a Boosting model
```{r}
set.seed(123)
tic()
boModel <- train(factor(Malaria.Test)~., data=train, method="ada", trControl=control)
toc()
boModel
bopred=predict(boModel,newdata = test)
bo_CM<- confusionMatrix(bopred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M10<- bo_CM$byClass[c(1, 2, 5, 7, 11)]
M10
#plotting confusion matrix
bo_CM$table
fourfoldplot(bo_CM$table, col=rainbow(4), main="Boosting Confusion Matrix")
#plot(varImp(boModel, scale=T))
```

# Tabulate your Results [xtable(measure.score, digits = 3)]
```{r}
measure <-round(data.frame(SVM = M1, RF = M2, DT = T2, LR = M3, KNN = M4, NN = M5, NB = M6, LDA = M7, LVQ = M8, Bagging = M9, Boosting = 10), 3) 
dim(measure)
rownames(measure)=c('Sensitivity', 'Specificity', 'Precision','F1-Score', 'Balanced Accuracy')
flextable(measure)
measure
```

# Collect all resamples and compare the models

```{r}
results <- resamples(list(SVM=SvmModel, 
                          RF=RFModel,
                          DT= DTModel,
                          LR=logRegModel,
                          knn=knnModel,
                          NB=NBModel,
                          LDA=LDAModel,
                          LVQ=LVQModel,
                          Bagging=bagModel,
                         Bo=boModel ))
# summarize the distributions of the results 
summary(results)
# boxplots of results
bwplot(results)
# dot plots of results
 dotplot(results)
```

```{r}

```

